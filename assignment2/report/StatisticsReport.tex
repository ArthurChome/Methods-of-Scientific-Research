\documentclass[14]{article}

% Used packages
\usepackage{apacite}
\usepackage[12pt]{moresize}
%\usepackage[numbers,sort&compress]{natbib}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage[explicit,noindentafter]{titlesec}
\usepackage[rightcaption]{sidecap}
\usepackage{caption}
\usepackage{float}
\usepackage{csquotes}
\usepackage{titling}
\usepackage{titlesec}
\usepackage{graphicx} %package to manage images
\graphicspath{{images/}}



\titlespacing\section{1pt}{12pt plus 0pt minus 0pt}{6pt plus 0pt minus 0pt}
\titlespacing\subsection{1pt}{12pt plus 4pt minus 2pt}{6pt plus 2pt minus 2pt}
\titlespacing\subsubsection{1pt}{12pt plus 4pt minus 2pt}{6pt plus 2pt minus 2pt}
\titlespacing\paragraph{1pt}{12pt plus 4pt minus 2pt}{6pt plus 2pt minus 2pt}

% New commands
\newcommand{\addsection}[3]{\addtocontents{toc}{\protect\contentsline{section}{\protect\numberline{#1}#2}{#3}}}
\newcommand{\addsubsection}[3]{\addtocontents{toc}{\protect\contentsline{subsection}{\protect\numberline{#1}#2}{#3}}}
\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\newcommand{\tab}[1]{\hspace{.5\textwidth}\rlap{#1}}
\newcommand{\Csh}{C{\lserif\#}}


\begin{document}
\author{\textbf{Faculty of Sciences and Bio-Engineering Sciences}\\[2\baselineskip]\newline\textbf{Arthur Chom√© - 0529279}}

\date{ \LARGE Assignment 2: Statistics}
\title{\vspace{-6cm}}%\underline{Everything is connected}}

\maketitle
\section{Question 1}
Given our student number 0529279 and the online calculator\footnote{\protect\url{https://ai.vub.ac.be/~bart/statsnumbers.html}}, were assigned the sets 4, 1, 4 and 3. We used IBM's SPSS Statistics package version 23\footnote{\protect\url{https://en.wikipedia.org/wiki/SPSS}} for the first 3 exercises and R\footnote{\protect\url{https://www.r-project.org/}} for the question 4. 
\newline
The used dataset --set 4 for the first question-- was gathered by way of an experiment measuring the resonance frequencies of two types of crystal oscillators (variables x1 and x2) needed to provide timing information for high performance hardware. 

\subsection{Make an appropriate plot to show how the data are distributed. Explain why you used this type of plot (2+2pts)}
The use of boxplots is a standardized way of displaying the distribution of data based on the five number summary. 
%Between the first and third quartile lies 50 percent of all data, between the maximum and minimum lies about 99.75 percent of all data.

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question1/Question1_Boxplot.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering
	%\caption{Given boxplots show that both variables have an evenly distributed range. Dataset x1 however has a wider distribution of its data with the maximum and minimum of the data being further away from the median than in dataset x2. } 
\end{figure}
\mbox{}\\ 
For boxplots, between the first and third quartile lies 50 percent of all data and between the maximum and minimum lies about 99.75 percent of all data. Datasets that have a normal distribution have symmetrical boxplots with the means being in the center. This is clearly the case for both datasets. Set x1 however has a wider distribution of its data with the maximum and minimum of the data being further away from the median than in dataset x2. 
\newline
The use of boxplots is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quartile, median, third quartile, and maximum. Using boxplots gives us a simple way of detecting symmetry and distribution of datasets and comparing them between each other. 

\subsection{Test whether the resonance frequencies of the two types could be identical (2 pts.) }
Based on the experiment's description, we want to see if the same variable applied on two different populations (e.g. different groups of oscillator crystals) makes for a difference of the means of variables. Because the variance of both datasets differs significantly, a normal T-test would not be reliable in doing so. 
\newline
This is why we settle for Welch's T-test because it is robust even when the sample size or the variance is unequal. In this case, an independent T-test is the best option. Our null hypothesis in this case is that the means of both data collections do not differ. 

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question1/question1_b.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering
	%\caption{Given boxplots show that both variables have an evenly distributed range. Dataset x1 however has a wider distribution of its data with the maximum and minimum of the data being further away from the median than in dataset x2. } 
\end{figure}

\paragraph{Null \& Alternative Hypothesis}\mbox{}\\
For this T-test, our null hypothesis is that no significant difference in means exists between the datasets x1 and x2 of the oscillator crystals. The alternative would be that a significant difference in means exists between the two. All hypothesis tests make use of a p-value\footnote{\protect\url{https://en.wikipedia.org/wiki/P-value}} to weigh the strength of the evidence. If the value is smaller than 0.05, we say there is strong evidence against the null hypothesis so it gets rejected.

\paragraph{Conclusion}\mbox{}\\
Because the p-value in this case is greater than 0.05 being 0.222, we cannot conclude that a significant difference exists and we accept our nullhypothesis: there is no significant difference in the means of both datasets.\\

\subsection{Also test whether they are different from the target frequency of 536 870 912 Hz. Explain why you used this test. (2 pts.)}
This test is used to see whether a dataset's mean is equal to a given average value. It can only be applied on a random sample with a normal distribution with the variance being --optionally-- unknown. Previous boxplots show our datasets to be normally distributed, we assume them to be randome samples.

\paragraph{Null \& Alternative Hypothesis}\mbox{}\\
Our null hypothesis here is that --for both crystal x1 and x2-- their mean does not differ significantly from the average target frequency of 536 870 912 Hz. If our p-value is smaller than 0.05, we would reject the null hypothesis and accept the alternative hypothesis: the difference is significant.

\paragraph{T-test Dataset x1}\mbox{}\\
After executing the T-test on dataset x1 of the crystal oscillators, we get the following tables. Our null hypothesis is that there is no significant difference between dataset x1's mean and given dataset average.

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question1/question1_x1_T.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
\end{figure}

\paragraph{Conclusion}\mbox{}\\
Because the p-value 0.367 is bigger than 0.05 we conclude there is no significant evidence against our null hypothesis and accept our claim that x1's mean is equal to 536 870 912 Hz.


\paragraph{T-test Dataset x2}\mbox{}\\
After executing the T-test on dataset x2 of the crystal oscillators, we get the following tables. Our null hypothesis is that there is no significant difference between dataset x2's mean and given the given average.

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question1/question1_x2_T.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
\end{figure}

\paragraph{Conclusion}\mbox{}\\
Because the p-value 0.324 is bigger than 0.05 we conclude there is no significant evidence against our null hypothesis and accept our claim that x2's mean does not differ significantly from 536 870 912 Hz.\\

\subsection{Would you trust these crystals in your application, and why (not)? (2 pts.)}
Because both dataset means do not differ significantly, are normally distributed and don't differ significantly from the average 536 870 912 Hz, we can trust both crystal types x1 and x2. 

\newpage
\section{Question 2}
We were assigned set 1 for this question, the data represents the number of times visitors bought something or simply left on two variants of a website (site1 and site2). The files whose name ends in a contain the results after 200 visits to each variant.

\begin{figure}[!htb]
	\includegraphics[width=0.7\textwidth]{img/question2/question2_a.PNG}
	\captionsetup{width=0.8\textwidth}
	\centering 
	\caption{The data seems to be a contigency table with the row representing the number of times whether something got bought or not in the two sites. } 
\end{figure}

\subsection{Do an appropriate test to see whether these data indicate a difference between the two sites. Explain why you used this test and give the effect size. (2+1+2 pts.)}
Because dataset A of our sites clearly represents a contingency table, we could opt to use either Fisher's Exact Test or Pearson's Chi-Squared Test. However, Fisher's test is more appropriate for small sample size and because the size of our test data (400 site visitors) is fairly low, we opt for Pearson Chi-Squared Test.

\paragraph{Null \& Alternative Hypothesis}\mbox{}\\
For this experiment, our null hypothesis is that there is no significant difference in buying behavior of the two sites. The alternative would be that there is significant difference between the two.

\paragraph{Conclusion}\mbox{}\\
For both the one-sided and two-sided variant of Fisher's Exact Test, we get p-value of 1,000 and 0,500 respectively. This means that we accept the null hypothesis that there is no significant buying behavior between the two sites.

\newpage
\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question2/Question2_Chi.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
	%\caption{} 
\end{figure}

\subsection{The files whose name ends in b contain data from a longer run (5000 visits) of the experiment. Do an appropriate test to see whether these data indicate a difference between the two sites. Explain why you used this test and give the effect size. (1+1+1 pts)}
For set B, we have data of about 500 site visitors. Because we have a bigger sample size than for set B, we now opt to use the non-parametric chi-squared test.

\begin{figure}[!htb]
	\includegraphics[width=0.7\textwidth]{img/question2/Question2_b.PNG}
	\captionsetup{width=0.8\textwidth}
	\centering 
	\caption{0,00 represents the times a site got visited and nothing got bought. 1,00 is the value for when a site got visited and something got bought.  } 
\end{figure}

\paragraph{Null \& Alternative Hypothesis}\mbox{}\\
Our null hypothesis is that there is no significant difference in buying behavior of the two sites (just a previous test).

\paragraph{Conclusion}\mbox{}\\
We can see that our significance level (p-value) when performing the Pearson Chi-Square test is equal to 0,000 hence less than 0.05. This is why we reject the null hypothesis that the buying behaviour of both sites is equal and we accept the alternate hypothesis that they are different.

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question2/Question2_b_chi.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
	%\caption{0,00 represents the times a site got visited and nothing got bought. 1,00 is the value for when a site got visited and something got bought.  } 
\end{figure}

\subsection{What did this teach you about the importance of sample size (a.k.a. "big data")? (2 pts)}
Sample size is directly related to a statistic's margin of error, or how accurate a statistic can be calculated to be. The bigger the sample size, the less chance there is to wrongfully accept the null hypothesis. 

\section{Question 3}
We are using set number 4 for the fourth question. It deals with the grade difference of a group of students that followed remedial lectures and a control group who did not.

\subsection{Investigate whether the remedial lectures had any effect, and explain what you did and why you did it (2+2 pts.)}
If the variances between the datasets would not differ significantly, we could have opted for a normal independent T-test. Because Levene's test for the equality of variances has a p-value of less than 0.05 (being 0.014), we conclude that both samples their variances differ significantly. Welch's test is a better alternative being robust for independent samples with different variances.

\begin{figure}[!htb]
	\includegraphics[width=0.8\textwidth]{img/question3/Levene.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
\end{figure}

\subsubsection{Null \& Alternative Hypothesis}%\mbox{}\\
Our null hypothesis here is that --for both groups-- the difference in study performance do not differ significantly from one another. If our p-value is smaller than 0.05, we would reject the null hypothesis and accept the alternative hypothesis: the difference is significant.

\begin{figure}[!htb]
	\includegraphics[width=0.9\textwidth]{img/question3/Welch.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
\end{figure}

\subsubsection{Conclusion}
Because Welch's test has a p-value of 0.198 (greater than 0.05), we accept the null hypothesis that the means of both dataset are equal. 

\subsection{What was the power of this experiment? What does this mean, and are you happy with this (and why)? (2+1 pts)}
When it comes to hypothesis testing, we can make type I and type II errors. Type I errors occur when we would reject the null hypothesis when it is in fact true, type II errors occur when we would accept the null hypothesis when it is false.
The power of a test is the probability of rejecting the null hypothesis, given it is false. It depends on several factors including the choice of Alpha and the sample size.

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question3/Power.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
\end{figure}

\paragraph{Discussion}\mbox{}\\
Our total sample size for this experiment is of 50. With with a p-value of 0.196 and an effect size of 0.035, we can say that there is a 25 \% chance that the detected difference in the test is really there. Hence, the probability of making a type II error would be 75 \%.
\newline 
Having a 75 \% change of accepting the null hypothesis when it is false makes the experiment very unreliable. We must enlarge the sample size to get more reliable results.

\subsection{How large would the groups need to be to achieve power 0.8? Explain. (2+1 pts)}
For this experiment, we can use Cohen's d (= effect size) --expected difference between the means of the target values between the experimental group and the control group-- as to determine the sample size using the power tables.
\newline 
The variables at work here are the power level, the means of the control and experiment group, the pooled standard deviation of the 2 groups and our alternative hypothesis.

\subsubsection{Significance, Power level \& Alternative Hypothesis}
These parameters for the sample size are the same for both the 2 groups of students. As is common practice in statistics, an experiment's significance level --chance of a type I error-- is set at 0.05 which is the significance we will go for. As was specified in the description, our power level is of 0.8 (80 \%). \\
For our null hypothesis, we agree that there is no difference between the means of the two groups. For our alternative hypothesis, we will go for the two-sided approach since this is the default used in Statistics. Whether we would go for a one or two-sided alternative hypothesis affects which table we would like to use.
\newpage
\subsubsection{Medians and Pooled Standard Deviation}
The inter-individual variability is the variation among the experimental subjects, expressed as the standard deviation. We have to calculate the standard deviation for both groups.

\begin{figure}[!htb]
	\includegraphics[width=0.8\textwidth]{img/question3/StandardDeviation.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
	\caption{We can see that the control group has a standard deviation of 0.9347 and the experimental group a standard deviation of 0.665. }
\end{figure}

\paragraph{Pooled Deviation}\mbox{}\\
A problem with Cohen's D is that it is supposed to be applied to 2 groups that have the same standard deviation which is not the case here. In this case, we need to take the average of the standard deviations using following formula:
\begin{figure}[!htb]
	\includegraphics[width=0.4\textwidth]{img/question3/PooledDeviation.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
\end{figure}\\
This gives a pooled deviation equal to: 0.8111.


\subsubsection{Cohen's D}
Using the means of 2 groups and the pooled deviation, we can compute Cohen's D and compare the results with a power table for a 2-sided alternative hypothesis.
We use following formula:
\begin{figure}[!htb]
	\includegraphics[width=0.3\textwidth]{img/question3/HomerD.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
\end{figure}\\
This gives the following result: 0.4291.

\subsubsection{Conclusion}
After searching the corresponding number in a power table with power 0.8 with a two-tailed alternative hypothesis and significance of 0.05, we need a sample size of 99 participants.


\section{Question 4}
We are using set number 3 for the fourth question as the data of the performance of two algorithms for deep learning. We have to perform binary logistic regression as to estimate relationships between a dependent variable (algorithm type and parameters) and binary independent variable (probability of success). To do so, we have to built up a model for the regression specifying a target variable (binary) and the covariate variables that decide the target's value.

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question4/question4_model_code.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
	\caption{To set up our model, we have to load up the data .txt file, give it training data, and specify that our result success variable is in function of the parameters variable and type variable. }
\end{figure}

\begin{figure}[!htb]
	\includegraphics[width=0.7\textwidth]{img/question4/question4_model_coef.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
	\caption{When looking at the p-values of the model, we can see the parameters variables is the most statistically significant for the outcome of the regression. }
\end{figure}

\subsection{Can you figure out whether one of the algorithms is better, controlling for the number of parameters? Explain how you determined this (3+3 pts).}
Binary Logistic regression was performed to measure the effect of parameters on successfully performing tasks for both algorithm types independently. For this, we changed all parameter entries to the median of each respective group and calculated the probability of success for each group/algorithm with the new parameter values.

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question4/question4_groups.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
	\caption{We use the same model for both experiments}
\end{figure}\mbox{}

\paragraph{Conclusion}\mbox{}\\
In our regression, we see that the probability for success for both algorithms when using the mean amount of parameters for each. Algorithm 1 has a probability of success of 0.4340497 and algorithm 2 of 0.4165832. This makes algorithm 2 slightly more performant.

\subsection{At which number of parameters does each of the algorithms have a 50 \% chance of succeeding? Explain how you calculated this. (2+2 pts).}
The model that we built for the logistic regression can be used again to calculate the probability of success in one specific case. For this purpose, we can use the predict function that takes in a model, training data (hence, all the data) and its return value (in this case response).

\begin{figure}[!htb]
	\includegraphics[width=1.0\textwidth]{img/question4/R_50_Percent_Chance.PNG}
	\captionsetup{width=1.0\textwidth}
	\centering 
	\caption{Finding the right amount of parameters for the algorithms was a process of trial-and-error. }
\end{figure}
	
\paragraph{Conclusion}\mbox{}\\
After trying different inputs, the closest we could get to a 50 \% probability of success would be 18050 parameters for algorithm 1 and and 18274 parameters for algorithm 2.


\end{document}